{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_xception.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paramphy/sem-image-classification-using-CNN/blob/master/keras_xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-hhlGEK5hv_"
      },
      "source": [
        "#Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP4CPHeffmzJ",
        "outputId": "0ef184ab-f108-499e-8012-1b2b07128b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkhZBmHi5r32"
      },
      "source": [
        "# Importing Required Libreries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IFfkXJifqVB"
      },
      "source": [
        "import keras \n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, RMSprop\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import Input, GlobalAveragePooling2D\n",
        "from keras import models\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2v7P4JX51qF"
      },
      "source": [
        "# Accuaring Data from Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDQMO8eeftVt",
        "outputId": "c3060d14-0f8d-46c8-d9e6-9b62fc2e700d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "image_size = (150, 150)\n",
        "batch_size = 32\n",
        " \n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/gdrive/My Drive/NANOML\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/gdrive/My Drive/NANOML\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18577 files belonging to 10 classes.\n",
            "Using 14862 files for training.\n",
            "Found 18577 files belonging to 10 classes.\n",
            "Using 3715 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1b3N7fy6FU_"
      },
      "source": [
        "# Visualization of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frqz9UCxfxP8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TuMbPsa6SOH"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNR3sG0jf101"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J46WFp-k6Xrm"
      },
      "source": [
        "## Visualizing Augmented data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJPu0-M6f6F-"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDhUzK5gGmx"
      },
      "source": [
        "#Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzIEpiG6gMgF"
      },
      "source": [
        "##Data Prefetch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnEUruQ5f91e"
      },
      "source": [
        "train_ds = train_ds.prefetch(buffer_size=32)\n",
        "val_ds = val_ds.prefetch(buffer_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RofN974dgTCC"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9KX-8OgBWF"
      },
      "source": [
        "model = tf.keras.applications.Xception(\n",
        "     include_top=True, weights = \"/content/gdrive/My Drive/Xception/xception_save_at_81.h5\", input_tensor=None, input_shape=(150,150,3),\n",
        "    pooling=\"max\", classes=10, classifier_activation = 'softmax'\n",
        ")\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zffDO_GygZUv"
      },
      "source": [
        "# Compiling Model And training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyTUOqLoggOK"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "logdir = os.path.join(\"/content/gdrive/My Drive/Xception/logs-xception/\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "csv_logger = tf.keras.callbacks.CSVLogger('/content/gdrive/My Drive/Xception/xception_training.log',append = True)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/Xception/9{epoch}.h5\"), tensorboard_callback,csv_logger\n",
        "]\n",
        "i = 0\n",
        "epochs = 8\n",
        "history = model.fit(train_ds, epochs=epochs,callbacks = callbacks, validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk1dzYyximx2"
      },
      "source": [
        "# Visualizing Training Data wit Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGnfkAh-gmHl"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHSIrvTFifEI"
      },
      "source": [
        "logs = \"/content/gdrive/My Drive/Xception/logs-xception\"\n",
        "%tensorboard --logdir logdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozivVlKp8LRQ"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}